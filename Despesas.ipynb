{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c523e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "datetime.today().strftime('Data: %Y-%m-%d Hora: %H:%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72e0ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "####################################### altera campos negativos entre parenteses para negativo numérico\n",
    "def transformar_em_negativo(valor):\n",
    "    if pd.isna(valor):\n",
    "        return valor\n",
    "    valor_str = str(valor)\n",
    "    valor_str = valor_str.replace('(', '-').replace(')', '')\n",
    "    return valor_str\n",
    "\n",
    "####################################### Função de formatação de valores \n",
    "def transformar_valor(valor):\n",
    "    if pd.isna(valor):\n",
    "        return valor\n",
    "    valor_str = str(valor)\n",
    "    valor_str = valor_str.replace('.', '').replace(',', '.')\n",
    "    return float(valor_str)\n",
    "\n",
    "\n",
    "\n",
    "####################################### Função para remover números e caracteres especiais, mantendo apenas as letras\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "\n",
    "## FUNÇÃO ABRE ARQUIVO PADRONIZAÇÃO E FAZ JOIN COM A TABELA DE ACORDO COM SEU SHEET\n",
    "\n",
    "def padronizacao(table,sheet,join_campo):\n",
    "    caminho_arquivo_padronizacao = \"//172.31.8.209/Files/CataliseInvestimentos/14. BigData Catalise/Staging/Despesas/De_Para/PADRONIZACAO.xls\"\n",
    "    \n",
    "    df = pd.read_excel(caminho_arquivo_padronizacao, sheet_name=sheet)\n",
    "    df = df.query(\"Grupo in ('DESPESA','COMPROMISSADA')\")\n",
    "    df['Descricao'].fillna('', inplace=True)\n",
    "    df['Descricao'] = df['Descricao'].apply(remove_special_characters) # elimina caracteres exp. e numeros\n",
    "    df['Descricao'] = df['Descricao'].str.lower()\n",
    "    df = df[['Descricao','Grupo','Categoria']]\n",
    "    \n",
    "    # elimina espaços \n",
    "    df_obj = df.select_dtypes('object')\n",
    "    df[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())\n",
    "    \n",
    "    df = df.drop_duplicates() \n",
    "    \n",
    "    df_merged = pd.merge(table, df, how='inner', on=f'{join_campo}')\n",
    "    return df_merged\n",
    "\n",
    "##### POR CodLancamento\n",
    "def padronizacaoCod(table,sheet,join_campo):\n",
    "    caminho_arquivo_padronizacao = \"//172.31.8.209/Files/CataliseInvestimentos/14. BigData Catalise/Staging/Despesas/De_Para/PADRONIZACAO.xls\"\n",
    "    \n",
    "    df = pd.read_excel(caminho_arquivo_padronizacao, sheet_name=sheet)\n",
    "    df = df.query(\"Grupo in ('DESPESA','COMPROMISSADA')\")\n",
    "    df['Descricao'].fillna('', inplace=True)\n",
    "    df['Descricao'] = df['Descricao'].apply(remove_special_characters) # elimina caracteres exp. e numeros\n",
    "    df['Descricao'] = df['Descricao'].str.lower()\n",
    "    \n",
    "    # elimina espaços \n",
    "    df_obj = df.select_dtypes('object')\n",
    "    df[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())\n",
    "    \n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    df_merged = pd.merge(table, df, how='inner', on=f'{join_campo}')\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d5e3e",
   "metadata": {},
   "source": [
    "# Alimenta o banco de dados\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "def FtDespesas(df):\n",
    "    # Função para inserir um pedaço do DataFrame na tabela MySQL\n",
    "    def insert_dataframe_to_mysql(df, connection, table_name):\n",
    "        cursor = connection.cursor()\n",
    "        for i, row in df.iterrows():\n",
    "            sql = f\"INSERT INTO {table_name} ({', '.join(row.index)}) VALUES ({', '.join(['%s'] * len(row))})\"\n",
    "            cursor.execute(sql, tuple(row))\n",
    "        connection.commit()\n",
    "        cursor.close()\n",
    "\n",
    "    # Conectar ao banco de dados MySQL\n",
    "    connection = mysql.connector.connect(\n",
    "        host='catalise-bi-dados-cluster.cluster-ciaao5zm9beh.sa-east-1.rds.amazonaws.com',\n",
    "        database='DW_CORPORATIVO',\n",
    "        user='admin',\n",
    "        password='yb222KraY7PTN0jbH7P3'\n",
    "    )\n",
    "\n",
    "    if connection.is_connected():\n",
    "        print('Conectado ao MySQL')\n",
    "\n",
    "        # Dividir o DataFrame em pedaços menores para evitar problemas de memória\n",
    "        tamanho_do_pedaço = 10000  # Ajuste conforme necessário\n",
    "        total_registros = len(df)\n",
    "\n",
    "        for i in range(0, total_registros, tamanho_do_pedaço):\n",
    "            pedaco_df = df.iloc[i:i+tamanho_do_pedaço]\n",
    "            insert_dataframe_to_mysql(pedaco_df, connection, 'Ft_Despesas')\n",
    "            print(f\"Registros inseridos: {i + len(pedaco_df)} / {total_registros}\")\n",
    "\n",
    "        print('Concluído')\n",
    "    else:\n",
    "        print('Falha na conexão ao MySQL')\n",
    "\n",
    "    # Fechar a conexão com o banco de dados\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82c5ff3",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a314f78e",
   "metadata": {},
   "source": [
    "# Singulare\n",
    "\n",
    "CashStatement.xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suprimir apenas DeprecationWarning e FutureWarning\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "\n",
    "def formata_dataframe(path,nome_do_sheet):\n",
    "    # Configurar pandas para ignorar SettingWithCopyWarning\n",
    "    pd.options.mode.chained_assignment = None  # ou 'warn' para mostrar os avisos\n",
    "\n",
    "    sheet = pd.read_excel(caminho_arquivo_excel, sheet_name=nome_do_sheet)\n",
    "    ## exclui as 3 primeiras linhas\n",
    "    df_singulare = sheet.iloc[3:]\n",
    "\n",
    "\n",
    "    ## cria uma lista com nomes de colunas da primeira linha de valores\n",
    "    colunas = []\n",
    "    for i in range(len(df_singulare.columns)):\n",
    "        x = df_singulare.iloc[0][i]\n",
    "        colunas.append(x) \n",
    "\n",
    "    ## Renomeia colunas usando a linha \n",
    "    df_singulare.columns = colunas\n",
    "\n",
    "    ## será usado este comapo apenas para o join \n",
    "    df_singulare['Descricao'] = df_singulare['Histórico'].apply(remove_special_characters) # elimina caracteres exp. e numeros\n",
    "\n",
    "    # ## Obtem nome do fundo \n",
    "    NmFundo = sheet[:3]['Demonstrativo de Caixa'][2].replace('Cliente: ', '')\n",
    "    df_singulare['NmFundo'] = NmFundo\n",
    "\n",
    "    ## exclui a primeira linha \n",
    "    df_singulare = df_singulare.iloc[1:]\n",
    "\n",
    "    # Preencher os valores NaN com o valor anterior\n",
    "    df_singulare['Data'] = df_singulare['Data'].fillna(method='ffill')\n",
    "\n",
    "    ## renomeia colunas para banco de dados\n",
    "    df_singulare = df_singulare[['Data','NmFundo','Histórico','Entradas','Saídas','Descricao']]\n",
    "    df_singulare['Descricao'] = df_singulare['Descricao'].str.lower()\n",
    "    \n",
    "    \n",
    "\n",
    "    #################################################### FAZ A NORMALIZAÇÃO COM O DE-PARA#####################################\n",
    "    # aplica a função\n",
    "    df_singulare_v2 = padronizacao(df_singulare,\"SINGULARE\",\"Descricao\")\n",
    "\n",
    "\n",
    "    ## converte data em datetime\n",
    "    date_columns = ['Data']\n",
    "    df_singulare_v2[date_columns] = df_singulare_v2[date_columns].apply(pd.to_datetime, format=\"%d/%m/%Y\")\n",
    "\n",
    "    # ## Valores numéricos\n",
    "    # ## valores negativos do arquivo representados entre parenteses\n",
    "\n",
    "    df_singulare_v2['Entradas'].fillna(0, inplace=True)\n",
    "    df_singulare_v2['Saídas'].fillna(0, inplace=True)\n",
    "    \n",
    "    columns_to_convert = ['Entradas', 'Saídas']\n",
    "\n",
    "    ## \n",
    "    for column in columns_to_convert:\n",
    "        df_singulare_v2[column] = df_singulare_v2[column].apply(transformar_em_negativo)\n",
    "\n",
    "    ## converte campos em float\n",
    "    for column in columns_to_convert:\n",
    "        df_singulare_v2[column] = df_singulare_v2[column].apply(transformar_valor)\n",
    "        \n",
    "    # ## troca NaN por zero\n",
    "    df_singulare_v2['VlrLancamento'] = df_singulare_v2['Entradas']+df_singulare_v2['Saídas']\n",
    "    \n",
    "\n",
    "    ## exclui coluna\n",
    "    df_singulare_v2.drop(columns=['Entradas','Saídas','Descricao'], inplace=True)\n",
    "    df_singulare_v2.columns = ['DtReferencia','NmFundo','Descricao','Grupo','Categoria','VlrLancamento']\n",
    "\n",
    "    return df_singulare_v2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################## aplica a função\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Caminho do diretório onde estão os arquivos\n",
    "diretorio = '//172.31.8.209/Files/CataliseInvestimentos/14. BigData Catalise/Staging/Despesas/SINGULARE/'\n",
    "\n",
    "# Loop para percorrer todos os arquivos no diretório\n",
    "for arquivo in os.listdir(diretorio):\n",
    "    # Verifica se o arquivo contém \"CashStatement\" no nome e termina com .xls ou .xlsx\n",
    "    if 'CashStatement' in arquivo and arquivo.endswith(('.xls', '.xlsx')):\n",
    "        caminho_arquivo_excel = os.path.join(diretorio, arquivo)\n",
    "        \n",
    "        # Ler o arquivo Excel\n",
    "#         excel_file = pd.ExcelFile(caminho_arquivo_excel)\n",
    "        # Ler o arquivo Excel usando o bloco \"with\"\n",
    "        with pd.ExcelFile(caminho_arquivo_excel) as excel_file:\n",
    "\n",
    "            # Obter os nomes das sheets\n",
    "            sheets = excel_file.sheet_names\n",
    "\n",
    "            # Lista para armazenar os dataframes\n",
    "            dfs = []\n",
    "\n",
    "            # Iterar sobre as sheets e aplicar a função de formatação\n",
    "            for i in range(len(sheets)):\n",
    "                df = formata_dataframe(caminho_arquivo_excel, sheets[i])\n",
    "                dfs.append(df)\n",
    "\n",
    "        # Concatenar todos os dataframes em um único dataframe\n",
    "        df_singulare = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        # Tratar dados\n",
    "        df_singulare.fillna('', inplace=True)\n",
    "        df_singulare['NmFundo'] = df_singulare['NmFundo'].str.strip()\n",
    "\n",
    "        # Tratativas para nomes específicos\n",
    "        df_singulare.loc[df_singulare['NmFundo'] == 'AGROFORTE FIDC - SUBORDINADA', 'NmFundo'] = 'FIDC AGROFORTE'\n",
    "        df_singulare.loc[df_singulare['NmFundo'] == 'APG FIDC - SUBORDINADA', 'NmFundo'] = 'FIDC APG'\n",
    "        df_singulare.loc[df_singulare['NmFundo'] == 'BRISTOL FIDC MULTISSETORIAL', 'NmFundo'] = 'FIDC BRISTOL'\n",
    "\n",
    "        # (Aqui você pode adicionar qualquer lógica de processamento ou salvamento para cada arquivo encontrado)\n",
    "        print(f\"Processado: {arquivo}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c2edbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_singulare.NmFundo.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f156195-70ad-44cf-bf80-a0eddb1e37e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_singulare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb888cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_singulare.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2f3237",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Min: {df_singulare.DtReferencia.min()}\\nMax: {df_singulare.DtReferencia.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ae400",
   "metadata": {},
   "source": [
    "### Insere no banco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d592ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "FtDespesas(df_singulare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d08fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpa pasta \n",
    "import os\n",
    " \n",
    "dir = diretorio\n",
    "for f in os.listdir(dir):\n",
    "    os.remove(os.path.join(dir, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0821f0",
   "metadata": {},
   "source": [
    " ________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2734742a",
   "metadata": {},
   "source": [
    "# Daycoval\n",
    "\n",
    "download.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652b6d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Desaativa avisos ####################################\n",
    "import warnings\n",
    "\n",
    "# Suprimir apenas DeprecationWarning e FutureWarning\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "#################################################################\n",
    "\n",
    "def formata_arq_daycoval(caminho_pasta,contador):\n",
    "\n",
    "    # Caminho da pasta com os arquivos\n",
    "    arquivos = os.listdir(caminho_pasta)\n",
    "\n",
    "    # Encontra arquivo \n",
    "    file = list(filter(lambda x: 'Demonstrativo de Caixa' in x, arquivos))\n",
    "    df_daycoval = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # Lê o arquivo CSV\n",
    "    df_daycoval = pd.read_csv(os.path.join(caminho_pasta, file[contador]), delimiter=';', encoding='latin-1', dtype='str')\n",
    "    df_daycoval=df_daycoval.iloc[::-1].reset_index(drop=True)\n",
    "\n",
    "    ## cria uma lista com nomes de colunas da primeira linha de valores\n",
    "    colunas = []\n",
    "    for i in range(len(df_daycoval.columns)):\n",
    "        x = df_daycoval.iloc[0][i]\n",
    "        colunas.append(x) \n",
    "\n",
    "    ## Renomeia colunas usando a linha \n",
    "    df_daycoval.columns = colunas\n",
    "\n",
    "    ## seleciona colunas importantes\n",
    "    df_daycoval = df_daycoval[['DtExibicao','NmCrt','Hist','TitCP','VlDeb','VlCred']]\n",
    "\n",
    "    ## renomeia\n",
    "    df_daycoval.columns = ['DtReferencia','NmFundo','Descricao','CodLancamento','VlrEntrada','VlrSaida']\n",
    "    \n",
    "#     df_daycoval['Descricao'] = df_daycoval['Descricao'].str.lower()\n",
    "\n",
    "\n",
    "    # elimina espaços \n",
    "    df_obj = df_daycoval.select_dtypes('object')\n",
    "    df_daycoval[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())\n",
    "\n",
    "    ## aplica join com o DE-PARA\n",
    "    df_daycoval = padronizacaoCod(df_daycoval,\"DAYCOVAL\",\"CodLancamento\")\n",
    "\n",
    "    df_daycoval = df_daycoval[['DtReferencia','NmFundo','Descricao_x','CodLancamento','VlrEntrada','VlrSaida','Grupo','Categoria']]\n",
    "    df_daycoval.rename(columns={'Descricao_x':\"Descricao\"}, inplace=True)\n",
    "    \n",
    "#     ## elimina caracteres exp. e numeros\n",
    "#     df_daycoval['Descricao'] = df_daycoval['Histórico'].apply(remove_special_characters) \n",
    "\n",
    "    \n",
    "    ## tratativas de colunas\n",
    "    ## Converte a coluna 'DtReferencia' para datetime\n",
    "    df_daycoval['DtReferencia'] = pd.to_datetime(df_daycoval['DtReferencia'], format=\"%d/%m/%Y\")# \"%d/%m/%Y\"\n",
    "\n",
    "    df_daycoval['VlrEntrada'].fillna(0, inplace=True)\n",
    "    df_daycoval['VlrSaida'].fillna(0, inplace=True)\n",
    "    \n",
    "    ## converter valores\n",
    "    columns_to_convert = ['VlrEntrada','VlrSaida']\n",
    "\n",
    "    ## converte campos em float\n",
    "    for column in columns_to_convert:\n",
    "        df_daycoval[column] = df_daycoval[column].apply(transformar_valor)\n",
    "\n",
    "\n",
    "    # Aplica a transformação para a coluna 'Saída'\n",
    "    df_daycoval.loc[df_daycoval['VlrSaida'] > 0, 'VlrSaida'] *= -1\n",
    "    \n",
    "    ## Calulo\n",
    "    df_daycoval['VlrLancamento'] = (df_daycoval['VlrEntrada'] + df_daycoval['VlrSaida'])\n",
    "    \n",
    "    ## campos vazios podem ser interpretados por funções como numérico\n",
    "    df_daycoval.fillna('', inplace=True)\n",
    "    \n",
    "    return df_daycoval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5990b8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openpyxl xlrd\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Configurar pandas para ignorar SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None  # ou 'warn' para mostrar os avisos\n",
    "\n",
    "# Caminho da pasta onde estão os arquivos\n",
    "caminho_pasta = \"//172.31.8.209/Files/CataliseInvestimentos/14. BigData Catalise/Staging/Despesas/DAYCOVAL\"\n",
    "\n",
    "# Lista os arquivos no diretório\n",
    "arquivos = os.listdir(caminho_pasta)\n",
    "\n",
    "# Filtra os arquivos que contêm 'Demonstrativo de Caixa' no nome\n",
    "arquivos_filtrados = [arq for arq in arquivos if 'Demonstrativo de Caixa' in arq]\n",
    "\n",
    "# Inicializar uma lista vazia para armazenar os DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Loop através da lista de arquivos filtrados\n",
    "for i, arquivo in enumerate(arquivos_filtrados):  # Usar enumerate para obter o índice\n",
    "    caminho_arquivo = os.path.join(caminho_pasta, arquivo)\n",
    "    \n",
    "    # Passa o caminho da pasta e o índice para a função\n",
    "    df_list.append(formata_arq_daycoval(caminho_pasta, i))  # Passa o caminho da pasta e o índice\n",
    "\n",
    "# Concatenar todos os DataFrames em um único DataFrame\n",
    "df_daycoval = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Excluir colunas indesejadas\n",
    "df_daycoval.drop(columns=['VlrEntrada', 'VlrSaida', 'CodLancamento'], inplace=True)\n",
    "\n",
    "# Normaliza Nome FIDC\n",
    "df_daycoval['NmFundo'] = df_daycoval['NmFundo'].str.strip()  # remove espaços\n",
    "\n",
    "# Dicionário para mapeamento dos nomes\n",
    "mapeamento_nomes = {\n",
    "    'BLUE ROCKET  SUB': 'FIDC B ROCKET',\n",
    "    'FIDC GLOBAL FUTU': 'FIDC GLOBAL FUTURO',\n",
    "    'IPE FIDC NP': 'FIDC IPE',\n",
    "    'PRIME AGRO FIDC': 'FIDC PRIME AGRO',\n",
    "    'ÁGIS 2 FIDC': 'FIDC AGIS 2',\n",
    "    'ÁGIS - FIDC NP': 'FIDC AGIS',\n",
    "    'ALBAREDO FIDC': 'FIDC ALBAREDO',\n",
    "    'SC FUNDO DE INVESTIMENTO EM DC - LR': 'FIDC SC',\n",
    "    'PRIME AGRO FIDC RL': 'FIDC PRIME AGRO',\n",
    "    'Z INVEST FIDC': 'FIDC Z INVEST',\n",
    "    'GLOBAL FUTURA FIDC RL': 'FIDC GLOBAL FUTURA',\n",
    "    'BASÃ- FUNDO DE INVESTIMENTO EM DIREITOS': 'FIDC BASÃ',\n",
    "    'IPE-FUNDO DE INVESTIMENTO EM DIREITOS NP': 'FIDC IPE',\n",
    "    'SDL - FUNDO DE INVESTIMENTO EM DIREITOS': 'FIDC SDL',\n",
    "    'K-FINANCE FIDC NP - SUBORDINADA': 'FIDC K-FINANCE',\n",
    "    'AGROCETE FUNDO DE INVESTIMENTO EM DIREIT': 'FIDC AGROCETE',\n",
    "    'AF6 FIDC - NP': 'FIDC AF6',\n",
    "    'BELL FUNDO DE INVESTIMENTO EM DIREITOS C': 'FIDC BELL',\n",
    "    'VERGINIA FUNDO DE INVESTIMENTO EM DIREIT': 'FIDC VERGINIA',\n",
    "    'CREDIAL BANK PAN FIDC': 'FIDC CREDIAL',\n",
    "    'KÉRDOS FUNDO DE INVESTIMENTO EM DIREITOS': 'FIDC KÉRDOS',\n",
    "    'CAPITALIZA FUNDO DE INVESTIMENTO EM DIRE': 'FIDC CAPITALIZA',\n",
    "    'FUTURO CAPITAL FUNDO DE INVESTIMENTO EM': 'FIDC FUTURO CAPITAL',\n",
    "    'BONTEMPO FIDC  - RL': 'FIDC BONTEMPO',\n",
    "    'VELSO - FUNDO DE INVESTIMENTO EM DIREITO': 'FIDC VELSO',\n",
    "    'GREENWOOD FIDC NP': 'FIDC GREENWOOD',\n",
    "    'NINE CAPITAL FIDC': 'FIDC NINE CAPITAL',\n",
    "    'BEFIC FUNDO DE INVESTIMENTO EM DIREITOS': 'FIDC BEFIC',\n",
    "    'CREDILOG - FUNDO DE INVESTIMENTO EM DIRE': 'FIDC CREDILOG',\n",
    "    'VISHNU FUNDO DE INVESTIMENTO EM DIREITOS': 'FIDC VISHNU',\n",
    "    'MASTRENN FIDC - RL': 'FIDC MASTRENN',\n",
    "    'BLUE ROCKET FIDC  - SUBORDINADA': 'FIDC BLUE ROCKET S',\n",
    "    'USECORP CATÁLISE FIDC': 'FIDC USECORP CATÁLISE',\n",
    "    'FIDC F2 BANK SUBORDINADA': 'FIDC F2 BANK',\n",
    "    'CREDILOG II - FUNDO DE INVESTIMENTO EM D': 'FIDC CREDILOG II',\n",
    "    'PRIME AGRO FIDC RL - SEN                ': 'FIDC PRIME AGRO SR',\n",
    "    '3RD - FUNDO DE INVESTIMENTO EM DIREITOS': 'FIDC 3RD',\n",
    "    'DBANK FUNDO DE INVESTIMENTO EM DIREITOS': 'FIDC DBANK',\n",
    "    'ARABAN FIC FIM RESPONSABILIDADE LIMITAD': 'FICFIM ARABAN',\n",
    "    'MACANAN FIC FIM RESPONSABILIDADE LIMITAD': 'FICFIM MACANAN',\n",
    "    'CONDOBEM FUNDO DE INVESTIMENTO EM DIREIT': 'FIDC CONDOBEM',\n",
    "    'TORONTO  FIDC RESP LIMITADA': 'FIDC TORONTO',\n",
    "    'MALBEC FIDC - RL': 'FIDC MALBEC',\n",
    "    'VITTRA FUNDO DE INVESTIMENTO EM DIREITOS': 'FIDC VITTRA',\n",
    "    'EJM FUNDO DE INVESTIMENTO EM DC - RL': 'FIDC EJM',\n",
    "    'SMT AGRO FUNDO DE INVESTIMENTO EM DIREIT': 'FIDC SMT AGRO',\n",
    "    'CATALISE FIC FIDC - RL':  'FICFIDC CATALISE',\n",
    "    'NR11 FUNDO DE INVESTIMENTO EM DIREITOS C': 'FIDC NR11',\n",
    "    'ANVERES FUNDO DE INVESTIMENTO EM DIREITO': 'FIDC ANVERES',\n",
    "    'ANIL FUNDO DE INVESTIMENTO EM DIREITOS C': 'FIDC ANIL'\n",
    "    \n",
    "}\n",
    "\n",
    "# Aplicar o mapeamento para normalizar os nomes\n",
    "df_daycoval['NmFundo'].replace(mapeamento_nomes, inplace=True)\n",
    "\n",
    "# Remover duplicados novamente após a normalização\n",
    "# df_daycoval.drop_duplicates(subset=['NmFundo', 'DtReferencia'], inplace=True)\n",
    "\n",
    "# Mostrar os dados únicos após a normalização\n",
    "df_daycoval[['NmFundo']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf0a213-356a-4f3a-9864-4630fec2a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daycoval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Min: {df_daycoval.DtReferencia.min()}\\nMax: {df_daycoval.DtReferencia.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a812928-da5c-4212-a827-51c4d29c501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daycoval\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64637eb6",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dec4944",
   "metadata": {},
   "source": [
    "### Insere no Banco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c731e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "FtDespesas(df_daycoval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff04a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpa pasta \n",
    "import os\n",
    " \n",
    "dir = caminho_pasta\n",
    "for f in os.listdir(dir):\n",
    "    os.remove(os.path.join(dir, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989bea0e",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3ae244",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1605929",
   "metadata": {},
   "source": [
    "# Master\n",
    "\n",
    "PTR_49002_20240705_141213.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32345350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formata_arq_master(caminho_pasta,contador):\n",
    "\n",
    "    arquivos = os.listdir(caminho_pasta)\n",
    "\n",
    "    # Encontra arquivo AGIS\n",
    "    file = list(filter(lambda x: 'PTR' in x, arquivos))\n",
    "    df_master = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # Lê o arquivo CSV\n",
    "    df_master = pd.read_csv(os.path.join(caminho_pasta, file[contador]), delimiter=';', encoding='latin-1', dtype='str')\n",
    "\n",
    "\n",
    "    # Converte a coluna 'DtReferencia' para datetime\n",
    "    df_master['DATALANCAMENTO'] = pd.to_datetime(df_master['DATALIQUIDACAO'], format=\"%d/%m/%Y\")\n",
    "\n",
    "    # converter valores\n",
    "    columns_to_convert = ['CREDITO', 'DEBITO']\n",
    "\n",
    "    ## \n",
    "    for column in columns_to_convert:\n",
    "        df_master[column] = df_master[column].apply(transformar_em_negativo)\n",
    "\n",
    "    ## converte campos em float\n",
    "    for column in columns_to_convert:\n",
    "        df_master[column] = df_master[column].apply(transformar_valor)\n",
    "\n",
    "    df_master['CREDITO'].fillna(0, inplace=True)\n",
    "    df_master['DEBITO'].fillna(0, inplace=True)\n",
    "    \n",
    "    # ## troca NaN por zero\n",
    "    df_master['VlrLancamento'] = df_master['CREDITO']+df_master['DEBITO']\n",
    "    \n",
    "\n",
    "    df_master.rename(columns={\"CODIGOLANCAMENTO\":'CodLancamento'}, inplace=True)\n",
    "\n",
    "    ## elimina caracteres exp. e numeros\n",
    "    df_master['Descricao'] = df_master['HISTORICO'].apply(remove_special_characters) \n",
    "\n",
    "    df_master['Descricao'].fillna('', inplace=True)\n",
    "    df_master['Descricao'] = df_master['Descricao'].str.lower()\n",
    "\n",
    "    # aplica a função join\n",
    "    df_master = padronizacao(df_master,\"MASTER\",\"Descricao\")\n",
    "\n",
    "    # ## renomeia\n",
    "    df_master = df_master[['DATALANCAMENTO','CARTEIRA','Descricao','VlrLancamento', 'Grupo','Categoria','HISTORICO']]\n",
    "    df_master.columns = ['DtReferencia','NmFundo','Descricao','VlrLancamento','Grupo','Categoria','HISTORICO']\n",
    "#     df_master.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return df_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf5e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar pandas para ignorar SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None  # ou 'warn' para mostrar os avisos\n",
    "\n",
    "caminho_pasta = \"//172.31.8.209/Files/CataliseInvestimentos/14. BigData Catalise/Staging/Despesas/MASTER\"\n",
    "\n",
    "# Caminho da pasta com os arquivos\n",
    "arquivos = os.listdir(caminho_pasta)\n",
    "\n",
    "# Encontra arquivo \n",
    "file = list(filter(lambda x: 'PTR' in x, arquivos))\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "# Inicializar uma lista vazia para armazenar os DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Loop através da lista de arquivos filtrados\n",
    "for i in range(len(file)):\n",
    "    # Chamar a função formata_arq_master e adicionar o DataFrame resultante à lista\n",
    "    df_list.append(formata_arq_master(caminho_pasta, i))\n",
    "\n",
    "# Concatenar todos os DataFrames em um único DataFrame\n",
    "df_master = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "df_master.drop(columns={'Descricao'}, inplace=True)\n",
    "\n",
    "df_master.rename(columns={\"HISTORICO\":'Descricao'}, inplace=True)\n",
    "# df_master.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "print(df_master[['NmFundo','DtReferencia']].drop_duplicates())\n",
    "\n",
    "print(f\"\\n{len(df_master)} Linhas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8e8510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar linhas duplicadas considerando colunas específicas\n",
    "duplicadas = df_master.duplicated(subset=['DtReferencia', 'NmFundo', 'Descricao', 'VlrLancamento', 'Grupo', 'Categoria'])\n",
    "\n",
    "# Exibir linhas duplicadas\n",
    "linhas_duplicadas = df_master[duplicadas]\n",
    "linhas_duplicadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6f2f96",
   "metadata": {},
   "source": [
    "### Insere no Banco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Min: {df_master.DtReferencia.min()}\\nMax: {df_master.DtReferencia.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524dcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master.query(\"NmFundo == 'FIDC TOPCRED' and DtReferencia == '2024-08-09'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aeb64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FtDespesas(df_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f8fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpa pasta \n",
    "import os\n",
    " \n",
    "dir = caminho_pasta\n",
    "for f in os.listdir(dir):\n",
    "    os.remove(os.path.join(dir, f))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
