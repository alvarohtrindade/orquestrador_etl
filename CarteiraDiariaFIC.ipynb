{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73e37f34-e0d4-4147-9f1c-e6339d11c3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d9cdc6-bc61-4964-94b4-b72f0e305f1d",
   "metadata": {},
   "source": [
    "## BTG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cb1ab95-e830-49b9-b992-76f1ed7b171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_directory(output_directory):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "def read_excel_file(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    temp_csv_path = os.path.join(os.path.dirname(file_path), \"temp.csv\")\n",
    "    df.to_csv(temp_csv_path, index=False, header=True)\n",
    "    df = pd.read_csv(temp_csv_path)\n",
    "    os.remove(temp_csv_path)\n",
    "    return df\n",
    "\n",
    "def process_portfolio_investido(df, nome_fundo, data):\n",
    "    try:\n",
    "        start_index = df[df[df.columns[0]] == 'Portfolio_Investido'].index[0]\n",
    "        end_index = df[df[df.columns[0]] == 'DESPESAS'].index[0]\n",
    "        portfolio_df = df.loc[start_index:end_index].iloc[1:-3].reset_index(drop=True)\n",
    "\n",
    "        new_column_names = portfolio_df.iloc[0].tolist()\n",
    "        portfolio_df.columns = new_column_names\n",
    "        portfolio_df = portfolio_df[1:].reset_index(drop=True)\n",
    "        portfolio_df.insert(0, 'Nome Fundo', nome_fundo)\n",
    "        portfolio_df.insert(1, 'Data', data)\n",
    "        portfolio_df = portfolio_df.iloc[:, :9].drop(columns=['ISIN', 'CNPJ', '% P.L.'])\n",
    "        portfolio_df['Classificacao'] = 'PORTFOLIO INVESTIDO'\n",
    "        \n",
    "        return portfolio_df, new_column_names\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar Portfolio Investido do fundo '{nome_fundo}': {e}\")\n",
    "        return None, None\n",
    "\n",
    "def process_titulos_publicos(df, nome_fundo, data, new_column_names):\n",
    "    try:\n",
    "        start_index = df[df[df.columns[0]] == 'Titulos_Publicos'].index[0]\n",
    "        end_index = df[df[df.columns[0]].isna() & (df.index > start_index)].index[0]\n",
    "        titulos_df = df.loc[start_index:end_index-1].iloc[1:].reset_index(drop=True)\n",
    "        titulos_df.columns = new_column_names\n",
    "        titulos_df['CNPJ'] = ''\n",
    "        titulos_df['Quantidade'] = ''\n",
    "        titulos_df['Quota'] = ''\n",
    "        titulos_df['Portfólio Inv.'] = titulos_df['Financeiro']\n",
    "        titulos_df['Financeiro'] = titulos_df['Var.Diária']\n",
    "        titulos_df.iloc[:, titulos_df.columns.get_loc('% P.L.')] = titulos_df.iloc[:, -2]\n",
    "        titulos_df = titulos_df.drop(columns=['ISIN']).iloc[:, :6].drop(index=0)\n",
    "        titulos_df.insert(0, 'Nome Fundo', nome_fundo)\n",
    "        titulos_df.insert(1, 'Data', data)\n",
    "        titulos_df = titulos_df.drop(columns=['CNPJ', '% P.L.'])\n",
    "        titulos_df['Classificacao'] = 'TITULOS PUBLICOS'\n",
    "        \n",
    "        return titulos_df\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar Títulos Públicos do fundo '{nome_fundo}': {e}\")\n",
    "        return None\n",
    "\n",
    "def process_acoes(df, nome_fundo, data, new_column_names):\n",
    "    try:\n",
    "        start_index = df[df[df.columns[0]] == 'Acoes'].index[0]\n",
    "        end_index = df[df[df.columns[0]].isna() & (df.index > start_index)].index[0]\n",
    "        acoes_df = df.loc[start_index:end_index-1].iloc[1:].reset_index(drop=True)\n",
    "        acoes_df.columns = new_column_names\n",
    "        acoes_df['Portfólio Inv.'] = acoes_df['Quantidade']\n",
    "        acoes_df['Quantidade'] = acoes_df['Quota']\n",
    "        acoes_df['Quota'] = acoes_df['Financeiro']\n",
    "        acoes_df['Financeiro'] = acoes_df['% P.L.']\n",
    "        acoes_df = acoes_df.drop(columns=['ISIN', 'CNPJ', '% P.L.']).iloc[:, :4].drop(index=0)\n",
    "        acoes_df.insert(0, 'Nome Fundo', nome_fundo)\n",
    "        acoes_df.insert(1, 'Data', data)\n",
    "        acoes_df['Classificacao'] = 'ACOES'\n",
    "        \n",
    "        return acoes_df\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar Ações do fundo '{nome_fundo}': {e}\")\n",
    "        return None\n",
    "\n",
    "def process_despesas(df, nome_fundo, data):\n",
    "    try:\n",
    "        start_index = df[df[df.columns[0]] == 'DESPESAS'].index[0]\n",
    "        despesas_df = df.loc[start_index:].iloc[1:, :4]\n",
    "        new_column_names = despesas_df.iloc[0].tolist()\n",
    "        despesas_df.columns = new_column_names\n",
    "        despesas_df = despesas_df[1:].reset_index(drop=True)\n",
    "        despesas_df = despesas_df.rename(columns={'Nome': 'Portfólio Inv.', 'Valor': 'Financeiro'})\n",
    "        despesas_df = despesas_df.drop(columns=['Data Início Vigência','Data Fim Vigência'])\n",
    "        despesas_df.insert(0, 'Nome Fundo', nome_fundo)\n",
    "        despesas_df.insert(1, 'Data', data)\n",
    "        despesas_df['Classificacao'] = 'DESPESAS'\n",
    "        \n",
    "        return despesas_df\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar Despesas do fundo '{nome_fundo}': {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_and_format_data(df):\n",
    "    try:\n",
    "        nome_fundo = df.iloc[5, 0].replace('_', ' ')\n",
    "        data = df.iloc[6, 1]\n",
    "        \n",
    "        portfolio_df, new_column_names = process_portfolio_investido(df, nome_fundo, data)\n",
    "        \n",
    "        if portfolio_df is None:\n",
    "            return data, None\n",
    "        \n",
    "        titulos_df = process_titulos_publicos(df, nome_fundo, data, new_column_names) if 'Titulos_Publicos' in df[df.columns[0]].values else None\n",
    "        acoes_df = process_acoes(df, nome_fundo, data, new_column_names) if 'Acoes' in df[df.columns[0]].values else None\n",
    "        despesas_df = process_despesas(df, nome_fundo, data) if 'DESPESAS' in df[df.columns[0]].values else None\n",
    "\n",
    "        # Formata as colunas diretamente no dataframe final\n",
    "        colunas_para_formatar = ['Quantidade', 'Quota', 'Financeiro']\n",
    "        final_df = pd.concat([portfolio_df, titulos_df, acoes_df, despesas_df], ignore_index=True)\n",
    "        for coluna in colunas_para_formatar:\n",
    "            if coluna in final_df.columns:\n",
    "                final_df[coluna] = final_df[coluna].astype(str).str.replace('.', ',')\n",
    "        final_df = final_df.astype(str).replace('nan', '').fillna('')\n",
    "        \n",
    "        return data, final_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao extrair e formatar dados do fundo '{nome_fundo}': {e}\")\n",
    "        return None, None\n",
    "\n",
    "def save_dataframes_by_date(dataframes_by_date, output_directory):\n",
    "    for date, dfs in dataframes_by_date.items():\n",
    "        concatenated_df = pd.concat(dfs, ignore_index=True)\n",
    "        output_file_name = f'CarteiraDiaria_Concatenated_{date.replace(\"/\", \"_\")}.csv'\n",
    "        output_file_path = os.path.join(output_directory, output_file_name)\n",
    "        concatenated_df.to_csv(output_file_path, index=False)\n",
    "        print(f\"Arquivos concatenados salvos em '{output_file_path}'\")\n",
    "\n",
    "def process_files(input_directory, output_directory):\n",
    "    create_output_directory(output_directory)\n",
    "    dataframes_by_date = {}\n",
    "    files = [f for f in os.listdir(input_directory) if f.endswith('.xlsx')]\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(input_directory, file)\n",
    "        df = read_excel_file(file_path)\n",
    "        data, final_df = extract_and_format_data(df)\n",
    "        \n",
    "        if data not in dataframes_by_date:\n",
    "            dataframes_by_date[data] = []\n",
    "        dataframes_by_date[data].append(final_df)\n",
    "    \n",
    "    save_dataframes_by_date(dataframes_by_date, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4898eb9-ba7f-4ad5-8b9e-293d37e222ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos concatenados salvos em '\\\\172.31.8.209\\Files\\CataliseInvestimentos\\11. Gestao (DCM, Riscos,Dados)\\11.12 Analises de Risco\\Fundos_monitorados_FIC\\Carteira Diária Locação\\OutputBTG\\CarteiraDiaria_Concatenated_11_03_2025.csv'\n",
      "Arquivos concatenados salvos em '\\\\172.31.8.209\\Files\\CataliseInvestimentos\\11. Gestao (DCM, Riscos,Dados)\\11.12 Analises de Risco\\Fundos_monitorados_FIC\\Carteira Diária Locação\\OutputBTG\\CarteiraDiaria_Concatenated_12_03_2025.csv'\n",
      "Arquivos concatenados salvos em '\\\\172.31.8.209\\Files\\CataliseInvestimentos\\11. Gestao (DCM, Riscos,Dados)\\11.12 Analises de Risco\\Fundos_monitorados_FIC\\Carteira Diária Locação\\OutputBTG\\CarteiraDiaria_Concatenated_13_03_2025.csv'\n",
      "Arquivos concatenados salvos em '\\\\172.31.8.209\\Files\\CataliseInvestimentos\\11. Gestao (DCM, Riscos,Dados)\\11.12 Analises de Risco\\Fundos_monitorados_FIC\\Carteira Diária Locação\\OutputBTG\\CarteiraDiaria_Concatenated_14_03_2025.csv'\n",
      "Arquivos concatenados salvos em '\\\\172.31.8.209\\Files\\CataliseInvestimentos\\11. Gestao (DCM, Riscos,Dados)\\11.12 Analises de Risco\\Fundos_monitorados_FIC\\Carteira Diária Locação\\OutputBTG\\CarteiraDiaria_Concatenated_17_03_2025.csv'\n",
      "Arquivos concatenados salvos em '\\\\172.31.8.209\\Files\\CataliseInvestimentos\\11. Gestao (DCM, Riscos,Dados)\\11.12 Analises de Risco\\Fundos_monitorados_FIC\\Carteira Diária Locação\\OutputBTG\\CarteiraDiaria_Concatenated_18_03_2025.csv'\n",
      "Arquivos concatenados salvos em '\\\\172.31.8.209\\Files\\CataliseInvestimentos\\11. Gestao (DCM, Riscos,Dados)\\11.12 Analises de Risco\\Fundos_monitorados_FIC\\Carteira Diária Locação\\OutputBTG\\CarteiraDiaria_Concatenated_19_03_2025.csv'\n",
      "Arquivos concatenados salvos em '\\\\172.31.8.209\\Files\\CataliseInvestimentos\\11. Gestao (DCM, Riscos,Dados)\\11.12 Analises de Risco\\Fundos_monitorados_FIC\\Carteira Diária Locação\\OutputBTG\\CarteiraDiaria_Concatenated_20_03_2025.csv'\n"
     ]
    }
   ],
   "source": [
    "# Definição dos diretórios de entrada e saída\n",
    "input_directory = r\"\\\\172.31.8.209\\Files\\CataliseInvestimentos\\11. Gestao (DCM, Riscos,Dados)\\11.12 Analises de Risco\\Fundos_monitorados_FIC\\Carteira Diária Locação\\CarteiraDiariaBTG\"\n",
    "output_directory = r\"\\\\172.31.8.209\\Files\\CataliseInvestimentos\\11. Gestao (DCM, Riscos,Dados)\\11.12 Analises de Risco\\Fundos_monitorados_FIC\\Carteira Diária Locação\\OutputBTG\"\n",
    "\n",
    "# Execução do processamento dos arquivos\n",
    "process_files(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b450860-aaef-4f5a-ae05-cfdf619dda24",
   "metadata": {},
   "source": [
    "## Daycoval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e556bd9-e697-4705-ad17-3783421f6e73",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 73\u001b[0m\n\u001b[0;32m     70\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNomeFundo\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNomeFundo\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(mapeamento_nomes)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Obtém a primeira data de emissão da coluna 'DtPosicao'\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m data_emissao \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDtPosicao\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Converte a data para o formato YYYYMMDD para usar no nome do arquivo\u001b[39;00m\n\u001b[0;32m     75\u001b[0m data_emissao_str \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(data_emissao, dayfirst\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\atrindade\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\atrindade\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1752\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m-> 1752\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\atrindade\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:1685\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1683\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[1;32m-> 1685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "# Define o caminho da pasta de entrada e de saída\n",
    "input_directory = r\"\\\\172.31.8.209\\Files\\CataliseInvestimentos\\11. Gestao (DCM, Riscos,Dados)\\11.12 Analises de Risco\\Fundos_monitorados_FIC\\Carteira Diária Locação\\CarteiraDiariaDaycoval\"\n",
    "output_directory = r\"\\\\172.31.8.209\\Files\\CataliseInvestimentos\\11. Gestao (DCM, Riscos,Dados)\\11.12 Analises de Risco\\Fundos_monitorados_FIC\\Carteira Diária Locação\\OutputDaycoval\"\n",
    "\n",
    "# Lista dos nomes antigos e novos para a coluna 'NomeFundo'\n",
    "nomes_antigos = ['AVANT FINTECH FIC FIM', 'GRUPO PRIME AGRO FIC FIM RESPONSABILIDAD', 'GOLIATH FUNDO DE INVESTIMENTO MULTIMERCA',\n",
    "                 'ARTANIS FUNDO DE INVESTIMENTO MULTIMERCA', 'ARCTURUS FIM CP IE', 'ABIATAR FIC DE FIM RESPONSABILIDADE LIMI', \n",
    "                 'NINE CAPITAL FIC DE FIM CP', 'MF HOLDING GROUP FIC FIM', 'BELLIN FIC FIM RESPONSABILIDADE LIMTADA', 'LC AZUL FIC DE FIM RESP LIMITADA']\n",
    "\n",
    "nomes_novos = ['FICFIM AVANT','FIC PRIME AGRO','FIC GOLIATH','FIC ARTANIS','FIC ARCTURUS','FIC ABIATAR','FICFIM NINE','FICFIM MF HOLDI', \n",
    "               'FICFIM BELLIN', 'FICFIM LC AZUL']  \n",
    "\n",
    "# Cria um dicionário de mapeamento dos nomes antigos para os novos nomes\n",
    "mapeamento_nomes = dict(zip(nomes_antigos, nomes_novos))\n",
    "\n",
    "codigos = [\n",
    "    '4471709', '8351082', '8367710', '8367779', '8618178', '8696977', '8745218', '8935297', '8935386', '8935564', '8935572', \n",
    "    '8881480', '8936315', '8936323', '9098470', '8935572', '9101063', '9674462'\n",
    "]\n",
    "\n",
    "# Cria o diretório de saída se não existir\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Itera sobre todos os arquivos CSV na pasta de entrada\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Caminho completo do arquivo CSV\n",
    "        csv_path = os.path.join(input_directory, filename)\n",
    "\n",
    "        # Lê o arquivo CSV em um DataFrame com encoding 'latin1' e separador ';'\n",
    "        df = pd.read_csv(csv_path, encoding='latin1', sep=\";\", header=None)\n",
    "\n",
    "        # Remove a última linha e a primeira coluna\n",
    "        df = df.iloc[:-1, 1:]\n",
    "\n",
    "        # Define os novos nomes das colunas com base na última linha do DataFrame\n",
    "        new_column_names = df.iloc[-1].tolist()\n",
    "        df.columns = new_column_names\n",
    "\n",
    "        # Filtra para manter apenas as linhas onde a primeira coluna é 'LayCrtDia' - desconsiderar linha do CDI\n",
    "        df = df[df.iloc[:, 0] == 'LayCrtDia']\n",
    "        df = df[df['CodigoCrt'].isin(codigos)]\n",
    "        \n",
    "        # Filtra para manter apenas as linhas onde a segunda coluna é 'V'\n",
    "        df = df[df.iloc[:, 1] == 'V']\n",
    "\n",
    "        # # Dropar as colunas que têm apenas valores NaN\n",
    "        # df = df.dropna(axis=1, how='all')\n",
    "        # df\n",
    "\n",
    "        # Define a lista de colunas a serem mantidas e seus novos nomes\n",
    "        colunas = ['NomeCrt', 'TpCrt', 'DataEmis', 'Nome', 'Espec', 'Qt', 'VlMrc', 'DataVenc', 'DataAplic', 'NoGrpN2']\n",
    "        novos_nomes_colunas = ['NomeFundo', 'TipoFundo', 'DtPosicao', 'Nome', 'Espec', 'Qnt', 'VlrMercado', 'DataVenc', 'DataAplic', 'Classificacao']\n",
    "\n",
    "        # Filtra o DataFrame para manter apenas as colunas especificadas\n",
    "        df = df[colunas]\n",
    "        \n",
    "        # Renomeia as colunas do DataFrame\n",
    "        df.columns = novos_nomes_colunas\n",
    "\n",
    "        # Converte os valores das colunas 'Nome' e 'Espec' para maiúsculas\n",
    "        df['Nome'] = df['Nome'].str.upper()\n",
    "        df['Espec'] = df['Espec'].str.upper()\n",
    "\n",
    "        # Remove caracteres em branco no final de todos os valores da tabela\n",
    "        df = df.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "        # Renomeia os valores na coluna 'NomeFundo' com base no mapeamento\n",
    "        df['NomeFundo'] = df['NomeFundo'].replace(mapeamento_nomes)\n",
    "        \n",
    "        # Obtém a primeira data de emissão da coluna 'DtPosicao'\n",
    "        data_emissao = df['DtPosicao'].iloc[0]\n",
    "        # Converte a data para o formato YYYYMMDD para usar no nome do arquivo\n",
    "        data_emissao_str = pd.to_datetime(data_emissao, dayfirst=True).strftime('%Y%m%d')\n",
    "        \n",
    "        # Renomeia o arquivo de entrada\n",
    "        new_input_filename = f\"CD_Daycoval_{data_emissao_str}.csv\"\n",
    "        new_csv_path = os.path.join(input_directory, new_input_filename)\n",
    "        os.rename(csv_path, new_csv_path)\n",
    "        \n",
    "        # Define o caminho do arquivo de saída\n",
    "        output_file_path = os.path.join(output_directory, f\"CarteiraDiaria_{data_emissao_str}.csv\")\n",
    "\n",
    "        # Salva o DataFrame no arquivo CSV com o nome especificado\n",
    "        df.to_csv(output_file_path, index=False)\n",
    "\n",
    "        print(f\"DataFrame salvo em '{output_file_path}' e arquivo de entrada renomeado para '{new_input_filename}'\")\n",
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
